# Chapter 14: S2 Integration Options ‚Äî In-Depth Analysis

This document provides a comprehensive analysis of three different approaches to integrating S2.dev with Durable Streams, examining architecture, implementation complexity, trade-offs, and migration paths.

---

## Table of Contents

1. [Overview & Context](#overview--context)
2. [Option 1: S2 as a Backing Store (Drop-in Storage Replacement)](#option-1-s2-as-a-backing-store-drop-in-storage-replacement)
3. [Option 2: S2 Direct + Durable Streams Client Protocol](#option-2-s2-direct--durable-streams-client-protocol)
4. [Option 3: S2 Direct + New Native S2 Client Library](#option-3-s2-direct--new-native-s2-client-library)
5. [Detailed Comparison Matrix](#detailed-comparison-matrix)
6. [Implementation Roadmaps](#implementation-roadmaps)
7. [Critical Decision Factors](#critical-decision-factors)
8. [Recommendations](#recommendations)

---

## Overview & Context

### Current Architecture Recap

```
Client ‚Üí CF CDN ‚Üí VPS Proxy ‚Üí CF Worker ‚Üí StreamDO ‚Üí DoSqliteStorage ‚Üí SQLite (hot log)
         ($0 HITs)  ($6/mo)                                           ‚Üí R2 (cold segments)
```

**Key cost optimizations:**
- **CDN request collapsing**: 99% HIT rate = 6.4B requests/month at $0
- **WebSocket Hibernation**: DO sleeps between writes, ~100x cost reduction
- **Current total**: $18/month for 10K concurrent readers

**Current protocol surface:**
- HTTP-based (PUT/POST/GET/HEAD/DELETE)
- Offset-based reads (opaque, lexicographically sortable)
- Producer fencing (epoch-based idempotency)
- Stream closure (durable EOF signal)
- Content-Type metadata per stream
- TTL/Expiry support
- SSE/Long-Poll/Catch-up read modes

### S2 Capabilities

S2 provides:
- Managed stream storage (no SQLite/R2 management)
- Append/read operations via REST API or streaming protocol
- Sequence numbers + timestamps per record
- SSE for tailing
- Retries with configurable policies
- Likely multi-region replication
- Higher write throughput (>200 batches/sec limitation of DOs)

**Critical unknowns:**
- Exact pricing model
- Latency characteristics (external service vs same-region DO)
- Stream closure support
- Content-Type metadata
- TTL/Expiry capabilities
- Producer fencing semantics

---

## Option 1: S2 as a Backing Store (Drop-in Storage Replacement)

### Architecture

```
Client ‚Üí CF CDN ‚Üí VPS Proxy ‚Üí CF Worker ‚Üí StreamDO ‚Üí S2Storage (adapter) ‚Üí S2 API
         (unchanged)          (unchanged) (unchanged) (NEW)                  (external)
```

**Key insight**: Keep the entire HTTP protocol layer, operation handlers, and sequencing logic. Only replace the storage backend.

### Implementation Details

#### Current Storage Abstraction

The `StreamStorage` interface (`packages/core/src/storage/types.ts`) defines all storage operations:

```typescript
interface StreamStorage {
  // Stream metadata
  getStream(streamId: string): Promise<StreamMeta | null>;
  insertStream(input: InsertStreamInput): Promise<void>;
  closeStream(streamId: string, closedAt: number, closedBy?: ProducerClosureInfo): Promise<void>;
  deleteStreamData(streamId: string): Promise<void>;
  
  // Producer state
  getProducer(streamId: string, producerId: string): Promise<ProducerState | null>;
  upsertProducer(streamId: string, producerId: string, input: UpsertProducerInput): Promise<void>;
  deleteProducer(streamId: string, producerId: string): Promise<void>;
  
  // Operations (messages)
  insertOpStatement(streamId: string, input: InsertOpInput): StorageStatement;
  selectOpsRange(streamId: string, startOffset: number, endOffset: number): Promise<OpRow[]>;
  selectOpsFrom(streamId: string, offset: number): Promise<OpRow[]>;
  deleteOpsThrough(streamId: string, endOffset: number): Promise<void>;
  
  // Segments
  insertSegment(input: InsertSegmentInput): Promise<void>;
  getLatestSegment(streamId: string): Promise<SegmentRow | null>;
  getSegmentCoveringOffset(streamId: string, offset: number): Promise<SegmentRow | null>;
  listSegments(streamId: string): Promise<SegmentRow[]>;
  
  // Batch operations (atomic transactions)
  batch(statements: StorageStatement[]): Promise<void>;
}
```

**Current implementation**: `DoSqliteStorage` backed by DO-local SQLite.

#### New S2 Implementation

Create `S2Storage` class implementing `StreamStorage`:

```typescript
export class S2Storage implements StreamStorage {
  private s2Client: S2;
  private basinName: string;
  
  constructor(s2Config: { accessToken: string; basin: string }) {
    this.s2Client = new S2({
      ...S2Environment.parse(),
      accessToken: s2Config.accessToken,
    });
    this.basinName = s2Config.basin;
  }
  
  async getStream(streamId: string): Promise<StreamMeta | null> {
    // Map to S2's checkTail() or equivalent metadata API
    const basin = this.s2Client.basin(this.basinName);
    const stream = basin.stream(streamId);
    
    try {
      const tail = await stream.checkTail();
      
      // Map S2 metadata to StreamMeta
      return {
        streamId,
        contentType: this.getContentType(tail), // May need custom headers
        closed: this.isStreamClosed(tail),       // S2 closure support?
        tailOffset: tail.seqNum,                 // Map seqNum to offset
        readSeq: 0,                              // S2 manages segments internally
        segmentStart: 0,
        segmentMessages: 0,
        segmentBytes: 0,
        // ... other fields from S2 metadata or defaults
      };
    } catch (error) {
      if (error instanceof S2Error && error.status === 404) {
        return null;
      }
      throw error;
    }
  }
  
  async insertStream(input: InsertStreamInput): Promise<void> {
    const basin = this.s2Client.basin(this.basinName);
    
    // S2's createStream equivalent
    await basin.streams.create({
      stream: input.streamId,
      // Map contentType, TTL, expiry to S2's metadata
      // This may require custom headers or metadata storage
    });
    
    // If initial body provided, append it
    if (input.body) {
      const stream = basin.stream(input.streamId);
      await stream.append(
        AppendInput.create([
          AppendRecord.bytes({ body: input.body }),
        ])
      );
    }
  }
  
  async insertOpStatement(streamId: string, input: InsertOpInput): StorageStatement {
    // Build a deferred operation that will be executed in batch()
    return {
      type: 'insert_op',
      streamId,
      input,
    };
  }
  
  async batch(statements: StorageStatement[]): Promise<void> {
    // Critical challenge: S2 doesn't have native batch transactions
    // Options:
    // 1. Execute sequentially (loses atomicity)
    // 2. Use S2's appendSession with pipelining
    // 3. Store metadata in a separate transactional store (KV? DO?)
    
    const basin = this.s2Client.basin(this.basinName);
    
    // Group statements by type
    const inserts = statements.filter(s => s.type === 'insert_op');
    const updates = statements.filter(s => s.type === 'update_meta');
    
    // Execute inserts via S2 append
    for (const stmt of inserts) {
      const stream = basin.stream(stmt.streamId);
      await stream.append(
        AppendInput.create([
          AppendRecord.bytes({
            body: stmt.input.body,
            headers: [
              ['producer-id', stmt.input.producerId || ''],
              ['producer-epoch', String(stmt.input.producerEpoch || 0)],
              ['producer-seq', String(stmt.input.producerSeq || 0)],
            ],
          }),
        ])
      );
    }
    
    // Metadata updates need a separate store
    // Could use KV or a separate DO for metadata coordination
  }
  
  async selectOpsRange(streamId: string, startOffset: number, endOffset: number): Promise<OpRow[]> {
    const basin = this.s2Client.basin(this.basinName);
    const stream = basin.stream(streamId);
    
    // Map offset to S2 sequence number
    const result = await stream.read({
      start: { from: { seqNum: startOffset } },
      stop: { to: { seqNum: endOffset } },
    }, { as: 'bytes' });
    
    // Map S2 records to OpRow
    return result.records.map(record => ({
      startOffset: record.seqNum,
      endOffset: record.seqNum + 1, // S2 uses single seqNum per record
      sizeBytes: record.body.length,
      streamSeq: null,
      producerId: this.getHeader(record, 'producer-id'),
      producerEpoch: parseInt(this.getHeader(record, 'producer-epoch') || '0'),
      producerSeq: parseInt(this.getHeader(record, 'producer-seq') || '0'),
      body: record.body,
      createdAt: record.appendedAt.getTime(),
    }));
  }
  
  // ... implement remaining StreamStorage methods
}
```

#### Critical Challenges

**1. Transactional Atomicity**

Current implementation uses SQLite transactions for atomic append:
```sql
BEGIN TRANSACTION;
INSERT INTO ops (start_offset, end_offset, body, ...) VALUES (...);
UPDATE stream_meta SET tail_offset = ?, segment_messages = ? WHERE stream_id = ?;
INSERT INTO producers (producer_id, epoch, seq, ...) VALUES (...) ON CONFLICT DO UPDATE;
COMMIT;
```

S2 doesn't provide cross-record transactions. **Solutions**:

- **Option A**: Accept eventual consistency (metadata lags behind appends)
  - Risk: Clients might read past tail_offset briefly
  - Mitigation: Use S2's sequence numbers as source of truth
  
- **Option B**: Store metadata in a separate transactional store
  - Use KV (limited transaction support) or separate DO for coordination
  - S2 for data, KV/DO for metadata (tailOffset, producer state, closed flag)
  - Adds complexity and latency
  
- **Option C**: Rely on S2's ordering guarantees
  - S2 assigns monotonic sequence numbers
  - Use checkTail() to get current tail offset
  - Producer state stored in KV with conditional writes

**2. Offset Encoding Mismatch**

Current system uses `readSeq_byteOffset` encoding:
- `readSeq`: Increments when segment rotates to R2
- `byteOffset`: Position within segment

S2 uses simple monotonic sequence numbers. **Solutions**:

- **Option A**: Map S2 seqNum directly to offset (abandon segment-based encoding)
  - Simpler, but breaks offset format compatibility
  
- **Option B**: Emulate segments purely for offset encoding
  - Track "virtual segments" in metadata
  - Increment readSeq every 1000 messages (or threshold)
  - Offset encoding stays compatible, but adds complexity

**3. Producer State Management**

Current system tracks producer state in SQLite for idempotency. S2 needs:

- **Option A**: Store producer state in KV
  - Separate `PRODUCER_STATE` KV namespace
  - Key: `${streamId}:${producerId}`
  - Value: `{ epoch, lastSeq, lastOffset, lastUpdated }`
  - Challenge: No cross-namespace transactions with S2
  
- **Option B**: Encode producer state in S2 record headers
  - Each append includes producer headers
  - On read, scan recent records to rebuild producer state
  - Challenge: Expensive for validation, no efficient lookup

**4. Stream Closure**

S2 docs don't mention explicit stream closure. **Solutions**:

- **Option A**: Store closure flag in metadata KV
  - Separate lookup on every append to check if closed
  - Performance impact
  
- **Option B**: Append a sentinel "close" message
  - Final message with special header: `stream-closed: true`
  - Readers detect closure by scanning for this marker
  - Challenge: Append after close must be rejected (need metadata check anyway)

**5. TTL/Expiry**

S2 likely has retention policies, but unclear if per-stream TTL is supported. **Solutions**:

- **Option A**: Store expiry in metadata KV, check on access
- **Option B**: Rely on S2's basin-level retention (less granular)
- **Option C**: Background job to delete expired streams

#### What You Gain

‚úÖ **Eliminate DO SQLite management**: No schema migrations, no storage quotas  
‚úÖ **Eliminate R2 segment rotation**: S2 handles cold storage internally  
‚úÖ **Likely better multi-region durability**: S2 is a managed service with replication  
‚úÖ **Higher write throughput potential**: No DO 200 batch/sec limitation per stream  
‚úÖ **Simpler cold storage**: No custom segment encoding/decoding  

#### What You Lose

‚ùå **Transactional guarantees**: S2 doesn't provide cross-record transactions  
‚ùå **Same-region latency**: External S2 API adds network hop (current: 10-50ms, S2: 50-150ms+)  
‚ùå **Offset encoding control**: May need to abandon segment-based format  
‚ùå **Direct storage cost visibility**: S2 pricing is bundled/opaque vs itemized Cloudflare costs  
‚ùå **Producer state co-location**: Need separate KV or DO for coordination  

#### Cost Impact

**Current (10K readers, 1 write/sec, 30 days)**:
- Worker requests (99% CDN HIT): $8/month
- DO requests: $4/month
- VPS proxy: $6/month
- **Total: $18/month**

**With S2 Storage (estimated)**:
- Worker requests (99% CDN HIT): $8/month (unchanged)
- DO requests: **$0** (DOs eliminated)
- VPS proxy: $6/month (unchanged)
- **S2 subscription: ???** (unknown, likely $50-200+/month for managed service)
- KV reads (metadata): $32.50/month (65M reads at $0.50/M)
- **Estimated total: $46.50 + S2 subscription = $96.50 - $246.50/month**

**Verdict**: Only cost-effective if S2 subscription is <$30/month (unlikely for managed service at scale).

---

## Option 2: S2 Direct + Durable Streams Client Protocol

### Architecture

```
Client (Durable Streams protocol) ‚Üí Protocol Adapter ‚Üí S2 API
                                    (translation layer)
```

**Key insight**: Build a lightweight HTTP server that translates Durable Streams HTTP protocol to S2 API calls. Clients use existing Durable Streams SDK, unaware of S2 backend.

### Implementation Details

#### Protocol Adapter Service

```typescript
import { S2 } from '@s2-dev/streamstore';
import { Hono } from 'hono';

export class DurableStreamsToS2Adapter {
  private s2: S2;
  private app: Hono;
  
  constructor(s2Config: { accessToken: string; basin: string }) {
    this.s2 = new S2({
      ...S2Environment.parse(),
      accessToken: s2Config.accessToken,
    });
    
    this.app = new Hono();
    this.setupRoutes();
  }
  
  private setupRoutes() {
    // PUT /v1/stream/{projectId}/{streamId} - Create stream
    this.app.put('/v1/stream/:projectId/:streamId', async (c) => {
      const { projectId, streamId } = c.req.param();
      const contentType = c.req.header('Content-Type') || 'application/octet-stream';
      const streamClosed = c.req.header('Stream-Closed') === 'true';
      const body = await c.req.arrayBuffer();
      
      const basin = this.s2.basin(projectId);
      
      try {
        // Create stream in S2
        await basin.streams.create({ stream: streamId });
        
        // Store metadata (content-type, TTL, closure) in KV
        await this.storeMetadata(projectId, streamId, {
          contentType,
          closed: streamClosed,
          ttlSeconds: this.parseTTL(c.req.header('Stream-TTL')),
          expiresAt: this.parseExpiry(c.req.header('Stream-Expires-At')),
        });
        
        // If body provided, append initial content
        if (body.byteLength > 0) {
          const stream = basin.stream(streamId);
          const ack = await stream.append(
            AppendInput.create([
              AppendRecord.bytes({ body: new Uint8Array(body) }),
            ])
          );
          
          return c.json({}, 201, {
            'Stream-Next-Offset': this.encodeOffset(ack.end.seqNum),
            'Stream-Closed': streamClosed ? 'true' : undefined,
          });
        }
        
        return c.json({}, 201, {
          'Stream-Next-Offset': this.encodeOffset(0),
        });
      } catch (error) {
        if (error instanceof S2Error && error.status === 409) {
          // Stream exists - verify metadata matches
          const meta = await this.getMetadata(projectId, streamId);
          if (meta.contentType === contentType) {
            return c.json({}, 200, {
              'Stream-Next-Offset': this.encodeOffset(meta.tailOffset),
            });
          }
          return c.json({ error: 'Stream exists with different configuration' }, 409);
        }
        throw error;
      }
    });
    
    // POST /v1/stream/{projectId}/{streamId} - Append
    this.app.post('/v1/stream/:projectId/:streamId', async (c) => {
      const { projectId, streamId } = c.req.param();
      const streamClosed = c.req.header('Stream-Closed') === 'true';
      const body = await c.req.arrayBuffer();
      
      // Check if stream is closed (metadata lookup)
      const meta = await this.getMetadata(projectId, streamId);
      if (meta.closed && !streamClosed) {
        return c.json(
          { error: 'Stream is closed' },
          409,
          { 'Stream-Closed': 'true', 'Stream-Next-Offset': this.encodeOffset(meta.tailOffset) }
        );
      }
      
      // Handle producer headers for idempotency
      const producerId = c.req.header('Producer-Id');
      const producerEpoch = parseInt(c.req.header('Producer-Epoch') || '0');
      const producerSeq = parseInt(c.req.header('Producer-Seq') || '0');
      
      if (producerId) {
        const dedup = await this.checkProducerDedup(
          streamId,
          producerId,
          producerEpoch,
          producerSeq
        );
        
        if (dedup.isDuplicate) {
          return c.json({}, 204, {
            'Stream-Next-Offset': this.encodeOffset(dedup.offset),
            'Producer-Epoch': String(dedup.epoch),
            'Producer-Seq': String(dedup.seq),
          });
        }
        
        if (dedup.error === 'stale_epoch') {
          return c.json({ error: 'Stale epoch' }, 403, {
            'Producer-Epoch': String(dedup.currentEpoch),
          });
        }
        
        if (dedup.error === 'sequence_gap') {
          return c.json({ error: 'Sequence gap' }, 409, {
            'Producer-Expected-Seq': String(dedup.expectedSeq),
            'Producer-Received-Seq': String(producerSeq),
          });
        }
      }
      
      // Append to S2
      const basin = this.s2.basin(projectId);
      const stream = basin.stream(streamId);
      
      const ack = await stream.append(
        AppendInput.create([
          AppendRecord.bytes({
            body: new Uint8Array(body),
            headers: producerId ? [
              ['producer-id', producerId],
              ['producer-epoch', String(producerEpoch)],
              ['producer-seq', String(producerSeq)],
            ] : [],
          }),
        ])
      );
      
      // Update producer state
      if (producerId) {
        await this.updateProducerState(streamId, producerId, {
          epoch: producerEpoch,
          lastSeq: producerSeq,
          lastOffset: ack.end.seqNum,
        });
      }
      
      // Update metadata
      await this.updateMetadata(projectId, streamId, {
        tailOffset: ack.end.seqNum,
        closed: streamClosed,
      });
      
      return c.json({}, 204, {
        'Stream-Next-Offset': this.encodeOffset(ack.end.seqNum),
        'Stream-Closed': streamClosed ? 'true' : undefined,
        'Producer-Epoch': producerId ? String(producerEpoch) : undefined,
        'Producer-Seq': producerId ? String(producerSeq) : undefined,
      });
    });
    
    // GET /v1/stream/{projectId}/{streamId} - Read
    this.app.get('/v1/stream/:projectId/:streamId', async (c) => {
      const { projectId, streamId } = c.req.param();
      const offsetParam = c.req.query('offset') || '0';
      const liveMode = c.req.query('live');
      
      const offset = this.decodeOffset(offsetParam);
      const meta = await this.getMetadata(projectId, streamId);
      
      if (!meta) {
        return c.json({ error: 'Stream not found' }, 404);
      }
      
      const basin = this.s2.basin(projectId);
      const stream = basin.stream(streamId);
      
      if (liveMode === 'sse') {
        // SSE tailing via S2
        const readSession = await stream.readSession({
          start: { from: { seqNum: offset } },
          stop: { waitSecs: 3600 }, // Keep alive for 1 hour
        }, { as: 'bytes' });
        
        // Transform to SSE format
        const sseStream = new ReadableStream({
          async start(controller) {
            const encoder = new TextEncoder();
            
            for await (const record of readSession) {
              // Send SSE event
              const data = btoa(String.fromCharCode(...record.body));
              const event = `data: ${JSON.stringify({ payload: data, encoding: 'base64' })}\n\n`;
              controller.enqueue(encoder.encode(event));
              
              // Send control message
              const control = `data: ${JSON.stringify({
                type: 'control',
                streamNextOffset: record.seqNum + 1,
                upToDate: false,
              })}\n\n`;
              controller.enqueue(encoder.encode(control));
            }
          },
        });
        
        return new Response(sseStream, {
          headers: {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
          },
        });
      }
      
      if (liveMode === 'long-poll') {
        // Long-poll with timeout
        const result = await stream.read({
          start: { from: { seqNum: offset } },
          stop: { limits: { count: 100 }, waitSecs: 4 },
        }, { as: 'bytes' });
        
        if (result.records.length === 0) {
          return c.json({}, 204, {
            'Cache-Control': 'no-store',
          });
        }
        
        const body = Buffer.concat(result.records.map(r => Buffer.from(r.body)));
        const nextOffset = result.records[result.records.length - 1].seqNum + 1;
        
        return c.body(body, 200, {
          'Content-Type': meta.contentType,
          'Stream-Next-Offset': this.encodeOffset(nextOffset),
          'Stream-Up-To-Date': String(nextOffset >= meta.tailOffset),
          'Stream-Closed': meta.closed ? 'true' : undefined,
          'Cache-Control': 'public, max-age=20',
        });
      }
      
      // Catch-up read
      const result = await stream.read({
        start: { from: { seqNum: offset } },
        stop: { limits: { count: 100 } },
      }, { as: 'bytes' });
      
      if (result.records.length === 0) {
        return c.body('', 200, {
          'Content-Type': meta.contentType,
          'Stream-Next-Offset': this.encodeOffset(offset),
          'Stream-Up-To-Date': 'true',
          'Stream-Closed': meta.closed ? 'true' : undefined,
          'Cache-Control': 'public, max-age=60',
        });
      }
      
      const body = Buffer.concat(result.records.map(r => Buffer.from(r.body)));
      const nextOffset = result.records[result.records.length - 1].seqNum + 1;
      
      return c.body(body, 200, {
        'Content-Type': meta.contentType,
        'Stream-Next-Offset': this.encodeOffset(nextOffset),
        'Stream-Up-To-Date': String(nextOffset >= meta.tailOffset),
        'Stream-Closed': meta.closed ? 'true' : undefined,
        'ETag': `"${offset}"`,
        'Cache-Control': 'public, max-age=60',
      });
    });
    
    // HEAD /v1/stream/{projectId}/{streamId} - Metadata
    this.app.head('/v1/stream/:projectId/:streamId', async (c) => {
      const { projectId, streamId } = c.req.param();
      const meta = await this.getMetadata(projectId, streamId);
      
      if (!meta) {
        return c.json({ error: 'Stream not found' }, 404);
      }
      
      return c.body('', 200, {
        'Content-Type': meta.contentType,
        'Stream-Next-Offset': this.encodeOffset(meta.tailOffset),
        'Stream-Closed': meta.closed ? 'true' : undefined,
      });
    });
    
    // DELETE /v1/stream/{projectId}/{streamId} - Delete
    this.app.delete('/v1/stream/:projectId/:streamId', async (c) => {
      const { projectId, streamId } = c.req.param();
      
      const basin = this.s2.basin(projectId);
      await basin.streams.delete({ stream: streamId });
      await this.deleteMetadata(projectId, streamId);
      
      return c.json({}, 204);
    });
  }
  
  // Helper methods for metadata storage (KV or separate service)
  private async storeMetadata(projectId: string, streamId: string, meta: any) {
    // Store in KV or metadata service
  }
  
  private async getMetadata(projectId: string, streamId: string) {
    // Fetch from KV or metadata service
  }
  
  private async updateMetadata(projectId: string, streamId: string, updates: any) {
    // Update KV or metadata service
  }
  
  private async deleteMetadata(projectId: string, streamId: string) {
    // Delete from KV or metadata service
  }
  
  private async checkProducerDedup(streamId: string, producerId: string, epoch: number, seq: number) {
    // Check producer state in KV or metadata service
  }
  
  private async updateProducerState(streamId: string, producerId: string, state: any) {
    // Update producer state in KV or metadata service
  }
  
  private encodeOffset(seqNum: number): string {
    // Map S2 seqNum to Durable Streams offset format
    return `0_${seqNum}`;
  }
  
  private decodeOffset(offset: string): number {
    // Parse Durable Streams offset to S2 seqNum
    if (offset === '-1' || offset === 'now') return -1;
    const parts = offset.split('_');
    return parseInt(parts[1] || '0');
  }
}
```

#### Deployment Options

**Option A: Cloudflare Worker**
```
Client ‚Üí CF Worker (Protocol Adapter) ‚Üí S2 API
```
- **Pros**: Leverages existing CF infrastructure, CDN caching still possible
- **Cons**: Worker ‚Üí S2 latency, request billing, no DO hibernation savings

**Option B: VPS/Docker Container**
```
Client ‚Üí VPS (Nginx ‚Üí Node.js/Hono adapter) ‚Üí S2 API
```
- **Pros**: Full control, low fixed cost ($6-12/month VPS)
- **Cons**: Single point of failure (unless load-balanced), manual scaling

**Option C: Serverless Function (AWS Lambda, Cloud Run)**
```
Client ‚Üí API Gateway ‚Üí Lambda (adapter) ‚Üí S2 API
```
- **Pros**: Auto-scaling, pay-per-request
- **Cons**: Cold starts, potentially higher cost than VPS at scale

#### What You Gain

‚úÖ **Client compatibility**: Existing Durable Streams SDKs work unchanged  
‚úÖ **Protocol preservation**: All Durable Streams semantics maintained  
‚úÖ **Flexible deployment**: Can run on any platform (CF Worker, VPS, Lambda, etc.)  
‚úÖ **S2 benefits**: Managed storage, higher throughput, multi-region replication  
‚úÖ **Easier migration**: Gradual stream-by-stream migration possible  

#### What You Lose

‚ùå **Additional translation layer**: More complexity, latency, failure points  
‚ùå **Metadata coordination**: Need separate storage for stream metadata, producer state  
‚ùå **CDN caching complexity**: Cache keys need careful design to preserve collapsing  
‚ùå **Deployment overhead**: Need to manage adapter service separately from S2  
‚ùå **Double billing**: Adapter infrastructure + S2 subscription  

#### Cost Impact

**With VPS Adapter + S2**:
- VPS adapter: $6-12/month
- S2 subscription: ??? (unknown)
- **Estimated: $56-212+/month** (assuming S2 is $50-200/month)

**With CF Worker Adapter + S2**:
- Worker requests (99% CDN HIT): $8/month
- VPS proxy (for CDN): $6/month
- S2 subscription: ???
- **Estimated: $64-214+/month**

**Verdict**: Only makes sense if S2's benefits (managed service, multi-region, throughput) outweigh 3-10x cost increase.

---

## Option 3: S2 Direct + New Native S2 Client Library

### Architecture

```
Client (New S2-native SDK) ‚Üí S2 API (direct)
```

**Key insight**: Abandon the Durable Streams protocol entirely. Build a new client library that speaks S2's native API, optimized for S2's strengths.

### Implementation Details

#### New Client Library: `@durable-streams/s2-client`

```typescript
import { S2, AppendInput, AppendRecord } from '@s2-dev/streamstore';

export class S2StreamClient {
  private s2: S2;
  private basin: string;
  
  constructor(config: { accessToken: string; basin: string }) {
    this.s2 = new S2({
      ...S2Environment.parse(),
      accessToken: config.accessToken,
    });
    this.basin = config.basin;
  }
  
  // Stream creation
  async createStream(streamId: string, options?: {
    contentType?: string;
    metadata?: Record<string, string>;
  }): Promise<void> {
    const basinClient = this.s2.basin(this.basin);
    await basinClient.streams.create({
      stream: streamId,
      // Pass through S2-specific options
    });
  }
  
  // Append operation (simplified, no producer headers)
  async append(streamId: string, data: Uint8Array | string): Promise<{ seqNum: bigint }> {
    const stream = this.s2.basin(this.basin).stream(streamId);
    
    const record = typeof data === 'string'
      ? AppendRecord.string({ body: data })
      : AppendRecord.bytes({ body: data });
    
    const ack = await stream.append(AppendInput.create([record]));
    
    return { seqNum: ack.end.seqNum };
  }
  
  // Batch append with S2's native session
  async appendSession(streamId: string, options?: {
    maxInflightBytes?: number;
  }): Promise<S2AppendSession> {
    const stream = this.s2.basin(this.basin).stream(streamId);
    const session = await stream.appendSession(options);
    
    return new S2AppendSession(session);
  }
  
  // Read operation (catch-up)
  async read(streamId: string, options: {
    fromSeqNum?: bigint;
    fromTimestamp?: Date;
    count?: number;
  }): Promise<S2ReadResult> {
    const stream = this.s2.basin(this.basin).stream(streamId);
    
    const result = await stream.read({
      start: options.fromSeqNum
        ? { from: { seqNum: options.fromSeqNum } }
        : { from: { timestamp: options.fromTimestamp! } },
      stop: { limits: { count: options.count || 100 } },
    }, { as: 'bytes' });
    
    return {
      records: result.records.map(r => ({
        seqNum: r.seqNum,
        timestamp: r.appendedAt,
        body: r.body,
        headers: r.headers,
      })),
      hasMore: result.records.length === (options.count || 100),
    };
  }
  
  // SSE tailing (via S2's native readSession)
  async tail(streamId: string, options: {
    fromSeqNum?: bigint;
    onMessage: (message: S2Message) => void;
    onError?: (error: Error) => void;
  }): Promise<() => void> {
    const stream = this.s2.basin(this.basin).stream(streamId);
    
    const readSession = await stream.readSession({
      start: options.fromSeqNum
        ? { from: { seqNum: options.fromSeqNum } }
        : { from: { tailOffset: 0 } },
      // No stop criteria = tail forever
    }, { as: 'bytes' });
    
    let cancelled = false;
    
    (async () => {
      try {
        for await (const record of readSession) {
          if (cancelled) break;
          
          options.onMessage({
            seqNum: record.seqNum,
            timestamp: record.appendedAt,
            body: record.body,
          });
        }
      } catch (error) {
        if (options.onError) {
          options.onError(error as Error);
        }
      }
    })();
    
    return () => { cancelled = true; };
  }
  
  // Stream metadata
  async getMetadata(streamId: string): Promise<S2StreamMetadata> {
    const stream = this.s2.basin(this.basin).stream(streamId);
    const tail = await stream.checkTail();
    
    return {
      tailSeqNum: tail.seqNum,
      // S2 may not provide closure, TTL, content-type in standard API
      // Would need custom metadata layer or accept reduced functionality
    };
  }
  
  // Stream deletion
  async deleteStream(streamId: string): Promise<void> {
    const basinClient = this.s2.basin(this.basin);
    await basinClient.streams.delete({ stream: streamId });
  }
}

// Helper classes
class S2AppendSession {
  constructor(private session: any) {}
  
  async submit(data: Uint8Array | string): Promise<{ seqNum: bigint }> {
    const record = typeof data === 'string'
      ? AppendRecord.string({ body: data })
      : AppendRecord.bytes({ body: data });
    
    const ticket = await this.session.submit(AppendInput.create([record]));
    const ack = await ticket.ack();
    
    return { seqNum: ack.seqNum() };
  }
  
  async close(): Promise<void> {
    await this.session.close();
  }
}

interface S2ReadResult {
  records: Array<{
    seqNum: bigint;
    timestamp: Date;
    body: Uint8Array;
    headers?: Array<[Uint8Array, Uint8Array]>;
  }>;
  hasMore: boolean;
}

interface S2Message {
  seqNum: bigint;
  timestamp: Date;
  body: Uint8Array;
}

interface S2StreamMetadata {
  tailSeqNum: bigint;
}
```

#### Usage Example

```typescript
import { S2StreamClient } from '@durable-streams/s2-client';

const client = new S2StreamClient({
  accessToken: process.env.S2_ACCESS_TOKEN!,
  basin: 'my-project',
});

// Create stream
await client.createStream('chat-messages');

// Append messages
await client.append('chat-messages', 'Hello, world!');
await client.append('chat-messages', new Uint8Array([1, 2, 3, 4]));

// Read messages
const result = await client.read('chat-messages', {
  fromSeqNum: 0n,
  count: 10,
});

for (const record of result.records) {
  console.log('SeqNum:', record.seqNum, 'Body:', record.body);
}

// Tail for new messages (SSE-like)
const cancel = await client.tail('chat-messages', {
  fromSeqNum: result.records[result.records.length - 1]?.seqNum || 0n,
  onMessage: (msg) => {
    console.log('New message:', msg.seqNum, msg.body);
  },
});

// Later: cancel()
```

#### What You Gain

‚úÖ **Simplicity**: No protocol translation, no adapter layer  
‚úÖ **Performance**: Direct S2 API access, lowest latency possible with S2  
‚úÖ **S2-native features**: Access all S2 capabilities (timestamps, headers, etc.)  
‚úÖ **Reduced operational overhead**: No adapter service to deploy/maintain  
‚úÖ **Cost efficiency**: No adapter infrastructure costs  
‚úÖ **Better S2 integration**: Leverage S2's strengths (multi-region, throughput, managed service)  

#### What You Lose

‚ùå **Protocol compatibility**: Existing Durable Streams clients won't work  
‚ùå **Breaking change**: Complete rewrite required for existing applications  
‚ùå **Lost features**: No producer fencing (unless S2 supports it), no stream closure (unless S2 adds it), no TTL/expiry (unless S2 supports it)  
‚ùå **Migration complexity**: Can't run old and new systems side-by-side  
‚ùå **CDN caching loss**: No edge worker caching layer (unless you rebuild it)  
‚ùå **Cloudflare optimizations**: Lose WebSocket Hibernation, DO-local storage, etc.  

#### Cost Impact

**Direct S2 (no adapter)**:
- S2 subscription: ??? (unknown, $50-200+/month estimated)
- **Total: $50-200+/month**

**With optional CDN proxy** (preserve caching benefits):
- CDN proxy (VPS): $6/month
- S2 subscription: ???
- **Total: $56-206+/month**

**Verdict**: Simplest approach, but only makes sense if:
1. You're willing to break compatibility with existing clients
2. S2 provides all needed features (closure, TTL, idempotency)
3. Cost increase is acceptable

---

## Detailed Comparison Matrix

| Aspect | **Option 1: S2 as Backing Store** | **Option 2: Protocol Adapter** | **Option 3: Native S2 Client** |
|--------|-----------------------------------|-------------------------------|-------------------------------|
| **Client Compatibility** | ‚úÖ Full (existing SDKs work) | ‚úÖ Full (existing SDKs work) | ‚ùå Breaking change (new SDK) |
| **Protocol Preservation** | ‚úÖ Complete | ‚úÖ Complete | ‚ùå New protocol (S2-native) |
| **Implementation Complexity** | üü° Medium (replace storage layer) | üî¥ High (adapter + metadata) | üü¢ Low (thin wrapper on S2 SDK) |
| **Latency** | üü° 50-150ms (external S2 + metadata) | üî¥ 100-200ms (adapter + S2) | üü¢ 50-100ms (direct S2) |
| **Operational Overhead** | üü° Medium (DO + KV + S2) | üî¥ High (adapter + KV + S2) | üü¢ Low (S2 only) |
| **Cost (estimated)** | üü° $96-246/month | üî¥ $64-214/month | üü¢ $50-200/month |
| **CDN Caching** | ‚úÖ Preserved (DO layer intact) | ‚úÖ Possible (requires design) | ‚ùå Lost (unless rebuilt) |
| **WebSocket Hibernation** | ‚úÖ Preserved | ‚ùå Lost (no DO) | ‚ùå Lost (no DO) |
| **Multi-Region Writes** | ‚úÖ If S2 supports | ‚úÖ If S2 supports | ‚úÖ If S2 supports |
| **Write Throughput** | ‚úÖ Higher (S2 likely >200/sec) | ‚úÖ Higher (S2 likely >200/sec) | ‚úÖ Higher (S2 likely >200/sec) |
| **Transactional Guarantees** | ‚ùå Reduced (no cross-record txns) | ‚ùå Reduced (metadata coordination) | ‚ùå S2's guarantees only |
| **Producer Fencing** | üü° Needs KV coordination | üü° Needs KV coordination | ‚ùå Lost (unless S2 supports) |
| **Stream Closure** | üü° Needs metadata flag | üü° Needs metadata flag | ‚ùå Lost (unless S2 supports) |
| **TTL/Expiry** | üü° Needs metadata + cleanup | üü° Needs metadata + cleanup | ‚ùå Lost (unless S2 supports) |
| **Migration Path** | üü¢ Gradual (stream-by-stream) | üü¢ Gradual (redirect traffic) | üî¥ All-or-nothing (breaking) |
| **Vendor Lock-in** | üî¥ High (CF DO + S2) | üü° Medium (adapter portable) | üü¢ Low (S2 only) |

### Feature Support Matrix

| Feature | **Current (DO)** | **Option 1** | **Option 2** | **Option 3** |
|---------|-----------------|-------------|-------------|-------------|
| Offset-based reads | ‚úÖ Yes | ‚úÖ Yes (emulated) | ‚úÖ Yes (translated) | ‚ùå No (seqNum) |
| Producer fencing (epoch/seq) | ‚úÖ Yes | üü° Via KV | üü° Via KV | ‚ùå Unknown |
| Stream closure (EOF) | ‚úÖ Yes | üü° Via metadata | üü° Via metadata | ‚ùå Unknown |
| Content-Type per stream | ‚úÖ Yes | üü° Via metadata | üü° Via metadata | ‚ùå Unknown |
| TTL/Expiry | ‚úÖ Yes | üü° Via metadata | üü° Via metadata | ‚ùå Unknown |
| SSE tailing | ‚úÖ Yes | ‚úÖ Yes (DO bridge) | ‚úÖ Yes (S2 SSE) | ‚úÖ Yes (S2 native) |
| Long-poll | ‚úÖ Yes | ‚úÖ Yes (DO queue) | ‚úÖ Yes (S2 + timeout) | üü° Manual (readSession) |
| WebSocket | ‚úÖ Yes (hibernation) | ‚úÖ Yes (DO bridge) | ‚ùå No | ‚ùå No |
| Segment rotation | ‚úÖ Yes (to R2) | ‚ùå S2 internal | ‚ùå S2 internal | ‚ùå S2 internal |
| CDN request collapsing | ‚úÖ Yes (99% HIT) | ‚úÖ Yes (DO layer) | üü° Needs design | ‚ùå No |
| ETag revalidation | ‚úÖ Yes | ‚úÖ Yes | üü° Possible | ‚ùå No |
| Transactional appends | ‚úÖ Yes (SQLite txn) | ‚ùå No | ‚ùå No | ‚ùå No |

---

## Implementation Roadmaps

### Option 1: S2 as Backing Store

**Phase 1: Prototype (2-4 weeks)**
1. Implement `S2Storage` class for core operations (getStream, insertStream, selectOpsRange)
2. Use in-memory Map for metadata/producer state (test only)
3. Run conformance tests against S2 backend
4. Measure latency, identify bottlenecks

**Phase 2: Metadata Layer (2-3 weeks)**
5. Design KV schema for stream metadata, producer state
6. Implement metadata coordination (conditional writes for atomicity)
7. Add TTL cleanup background job
8. Test producer fencing with KV coordination

**Phase 3: Production Hardening (3-4 weeks)**
9. Error handling (S2 API failures, metadata inconsistencies)
10. Monitoring/alerting (S2 latency, metadata lag, dedup failures)
11. Load testing (10K readers, sustained writes)
12. Cost analysis (actual S2 pricing at scale)

**Phase 4: Migration (4-6 weeks)**
13. Build stream migration tool (DO ‚Üí S2)
14. Gradual rollout (test streams, then production)
15. Monitor for issues (latency spikes, consistency bugs)
16. Full cutover or hybrid deployment

**Total: 11-17 weeks**

---

### Option 2: Protocol Adapter

**Phase 1: Core Adapter (3-5 weeks)**
1. Implement Hono routes for PUT/POST/GET/HEAD/DELETE
2. Map Durable Streams headers to S2 API calls
3. Basic offset encoding (S2 seqNum ‚Üî DS offset)
4. Test with Durable Streams SDK

**Phase 2: Metadata & Producer State (2-3 weeks)**
5. Design KV schema for metadata (contentType, closed, TTL)
6. Implement producer fencing (checkProducerDedup, updateProducerState)
7. Add closure flag checks on append
8. Test idempotency end-to-end

**Phase 3: Real-time Modes (2-3 weeks)**
9. Implement SSE translation (S2 readSession ‚Üí EventSource)
10. Implement long-poll (S2 read with waitSecs)
11. Test with clients expecting live updates

**Phase 4: CDN Integration (2-3 weeks)**
12. Design cache keys for collapsing (offset + cursor)
13. Set Cache-Control headers correctly
14. Test CDN HIT rate (99% target)
15. Deploy VPS/Worker proxy

**Phase 5: Production Deployment (3-4 weeks)**
16. Choose deployment platform (CF Worker, VPS, Lambda)
17. Setup monitoring/alerting
18. Load test at scale
19. Gradual traffic migration

**Total: 12-18 weeks**

---

### Option 3: Native S2 Client

**Phase 1: Client Library (2-3 weeks)**
1. Implement `S2StreamClient` wrapper class
2. Add TypeScript types for responses
3. Write unit tests
4. Publish to npm

**Phase 2: Documentation & Examples (1-2 weeks)**
5. API reference docs
6. Migration guide from Durable Streams SDK
7. Example applications (chat, event log, etc.)
8. Comparison table (what changed)

**Phase 3: Application Migration (varies)**
9. Rewrite applications to use new SDK
10. Test thoroughly (integration, e2e)
11. Deploy updated applications

**Phase 4: (Optional) CDN Proxy (3-4 weeks)**
12. Build Cloudflare Worker that caches S2 reads
13. Test collapsing behavior
14. Deploy and monitor HIT rate

**Total: 6-9 weeks (library only) or 9-13 weeks (with CDN proxy)**

---

## Critical Decision Factors

### 1. **S2 Pricing**

**Must know before deciding:**
- What does S2 cost at target scale (10K readers, 1 write/sec, 30 days)?
- Is pricing per-stream, per-GB, per-request, or bundled?
- Are there free tier limits or minimum commitments?

**Decision threshold:**
- If S2 < $50/month ‚Üí Option 3 (native) makes sense
- If S2 $50-100/month ‚Üí Option 2 (adapter) preserves more value
- If S2 >$100/month ‚Üí Option 1 (backing store) or stay with DO

### 2. **S2 Feature Support**

**Critical features to verify:**
- Stream closure (EOF signal) ‚Üí Required for finite streams
- Per-stream metadata (content-type, TTL) ‚Üí Required for multi-tenant
- Producer fencing (idempotency) ‚Üí Required for exactly-once semantics
- SSE tailing ‚Üí Required for real-time UX

**Decision matrix:**
| Features Supported | Best Option |
|-------------------|------------|
| All features | Option 3 (native, simplest) |
| Most features, missing 1-2 | Option 2 (adapter can fill gaps) |
| Few features | Option 1 (keep DO layer, use S2 for storage only) |

### 3. **Latency Requirements**

**Current: 10-50ms write ACK (same-region DO)**

S2 latency depends on:
- S2's infrastructure (where are their servers?)
- Network path (client ‚Üí adapter ‚Üí S2 or client ‚Üí S2)
- S2's consistency model (synchronous replication?)

**Decision threshold:**
- If S2 ACK latency <100ms ‚Üí acceptable for most use cases
- If S2 ACK latency 100-200ms ‚Üí may hurt real-time UX (collaborative editing)
- If S2 ACK latency >200ms ‚Üí stay with DO for latency-sensitive streams

### 4. **CDN Caching Requirement**

**Current: 99% CDN HIT rate = 6.4B requests/month at $0**

Without CDN caching:
- 6.5B requests/month hit the origin (adapter or S2)
- At $0.30/M Worker requests: **$1,950/month** just for Worker execution
- At typical API gateway pricing: **$3,000+/month**

**Decision:**
- If you **must** preserve CDN caching ‚Üí Option 1 or Option 2 with CF Worker adapter
- If CDN caching is **nice to have** ‚Üí Option 2 or Option 3, accept higher cost
- If workload is **write-heavy, not read-heavy** ‚Üí CDN caching less critical, Option 3 simplest

### 5. **Migration Risk Tolerance**

**Option 1 (backing store)**: Medium risk
- Keep protocol surface, change backend
- Gradual migration possible
- Rollback path exists (keep DO code)

**Option 2 (adapter)**: Medium-high risk
- Additional layer (adapter) can fail
- Metadata coordination complexity
- Rollback requires keeping adapter running

**Option 3 (native)**: High risk
- Complete protocol change
- All clients must migrate
- No rollback path (unless you rebuild DO system)

**Decision:**
- Low risk tolerance ‚Üí Option 1 or stay with DO
- Medium risk tolerance ‚Üí Option 2
- High risk tolerance + clear S2 benefits ‚Üí Option 3

---

## Recommendations

### Scenario A: You're Starting Fresh (New Project)

**Recommendation: Option 3 (Native S2 Client)**

**Why:**
- No existing Durable Streams clients to maintain compatibility with
- Simplest architecture (no adapter layer)
- Lowest cost (no adapter infrastructure)
- Get full benefit of S2's features

**Caveat:** Verify S2 supports your must-have features (closure, TTL, idempotency).

---

### Scenario B: You Have Existing Durable Streams Applications

**Recommendation: Option 2 (Protocol Adapter) or Stay with DO**

**Why:**
- Preserves client compatibility (no application rewrites)
- Gradual migration possible
- Option to fall back to DO if S2 doesn't work out

**When to choose Option 2 over staying with DO:**
- You need >200 batches/sec write throughput per stream
- You need multi-region write support
- S2 pricing is <5x current cost ($90/month or less)
- Operational simplicity is worth the cost increase

---

### Scenario C: Read-Heavy, Cost-Sensitive

**Recommendation: Stay with DO or Option 1 (S2 Backing Store)**

**Why:**
- CDN request collapsing is critical for cost (99% HIT = $0)
- Option 1 preserves the DO layer and CDN caching
- Option 2/3 would require rebuilding CDN proxy

**If you choose Option 1:**
- Accept increased latency (metadata coordination overhead)
- Budget for KV reads ($32.50/month at 65M reads)
- Verify S2 pricing is competitive with current $18/month

---

### Scenario D: Write-Heavy, Multi-Region

**Recommendation: Option 3 (Native S2 Client)**

**Why:**
- Write-heavy means CDN caching less critical (fewer reads)
- Multi-region writes likely require managed service (DO is single-region)
- Simplest architecture for this use case

**If S2 doesn't support required features:**
- Fall back to Option 2 (adapter can add closure, TTL via metadata)
- Or stay with DO and accept single-region writes

---

### Scenario E: Cost is Critical, Latency is Flexible

**Recommendation: Stay with DO**

**Why:**
- Current $18/month is hard to beat with managed service
- S2 likely 3-10x more expensive
- If latency <200ms is acceptable, current architecture already delivers

**When to reconsider:**
- Free tier or steep discounts available from S2
- Operational overhead of DO becomes unmanageable
- Need features DO can't provide (multi-region, higher throughput)

---

## Next Steps

### 1. Gather S2 Information

**Before any decision:**
- Contact S2 sales/support for pricing at your scale
- Test S2's latency from your target regions
- Verify feature support (closure, TTL, idempotency, metadata)
- Understand S2's consistency model (sync vs async replication)

### 2. Build Proof-of-Concept

**Option 1 PoC (2 weeks):**
- Implement `S2Storage` for core operations
- Use in-memory metadata (no KV yet)
- Run conformance tests
- Measure latency and cost

**Option 2 PoC (3 weeks):**
- Build minimal adapter (PUT/POST/GET only)
- Use in-memory metadata
- Test with Durable Streams SDK
- Measure end-to-end latency

**Option 3 PoC (1 week):**
- Build `S2StreamClient` wrapper
- Write example app
- Test append/read/tail operations
- Measure latency

### 3. Compare Costs

**Build spreadsheet:**
| Scenario | Current (DO) | Option 1 (S2 Backing) | Option 2 (Adapter) | Option 3 (Native) |
|----------|-------------|----------------------|-------------------|------------------|
| Infrastructure | $18/mo | $96-246/mo | $64-214/mo | $50-200/mo |
| Engineering time | 0 (already built) | 11-17 weeks | 12-18 weeks | 6-13 weeks |
| Migration risk | N/A | Medium | Medium-High | High |

### 4. Make Decision

**Decision tree:**
```
Do you need multi-region writes or >200 batches/sec?
‚îú‚îÄ Yes ‚Üí Is S2 cost <5x current?
‚îÇ   ‚îú‚îÄ Yes ‚Üí Do you have existing clients?
‚îÇ   ‚îÇ   ‚îú‚îÄ Yes ‚Üí Option 2 (Adapter)
‚îÇ   ‚îÇ   ‚îî‚îÄ No ‚Üí Option 3 (Native)
‚îÇ   ‚îî‚îÄ No ‚Üí Stay with DO
‚îî‚îÄ No ‚Üí Is operational overhead unmanageable?
    ‚îú‚îÄ Yes ‚Üí Is S2 cost acceptable?
    ‚îÇ   ‚îú‚îÄ Yes ‚Üí Option 1 (Backing Store)
    ‚îÇ   ‚îî‚îÄ No ‚Üí Stay with DO, optimize ops
    ‚îî‚îÄ No ‚Üí Stay with DO
```

---

## Conclusion

All three S2 integration options are **technically viable**, but each has different trade-offs:

| Option | Best For | Avoid If |
|--------|----------|----------|
| **1: S2 Backing Store** | You want S2 benefits but must preserve protocol | Cost is critical, latency must be <50ms |
| **2: Protocol Adapter** | Existing clients, need compatibility, gradual migration | Cost is critical, team is small |
| **3: Native S2 Client** | New project, simplicity > compatibility | Have existing clients, breaking changes unacceptable |

**The current Durable Objects architecture is hyper-optimized for cost ($18/month for 10K readers)**. S2 is a great managed service, but it's unlikely to match that cost efficiency. The decision should be driven by:

1. **Features you can't get with DO** (multi-region, higher throughput)
2. **Operational overhead reduction** (no SQLite, R2, rotation management)
3. **Engineering time savings** (managed service vs self-hosted)

If those benefits outweigh 3-10x cost increase, S2 makes sense. Otherwise, stick with the current DO architecture.
